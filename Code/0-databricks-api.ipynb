{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this notebook is to play a bit with the workspace API in databricks to understand how to use it, and eventually, how to push files from github up to the workspace, and how to pull down from it.\n",
    "\n",
    "The code will use the `requests` library in order to do the REST calls.\n",
    "\n",
    "The API documentation is available [here](https://docs.azuredatabricks.net/api/latest/index.html).\n",
    "\n",
    "Specifically, we will focus on the [Workspace API](https://docs.azuredatabricks.net/api/latest/workspace.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "This uses two libraries\n",
    "\n",
    "- `python-dotenv` to load and manage access tokens (see authentication below)\n",
    "- `requests` to send the REST calls\n",
    "\n",
    "Both are installed into a conda environment by running the following command in the same directory as this notebook:\n",
    "\n",
    "```\n",
    "conda env create -f dbapi_conda.yml\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - just reading the workspace\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Authentication\n",
    "\n",
    "This will work through an example using a personal access token.\n",
    "\n",
    "In order to use this approach, you need to generate a databricks personal access token.\n",
    "\n",
    "To do so, do the following:\n",
    "\n",
    "- Click on the `user` icon on the top right\n",
    "- Click on the `User Settings` menu item\n",
    "- Select the `Access Tokens` tab\n",
    "- Click on the `Generate Access Token`\n",
    "- Fill in the requested fields\n",
    "\n",
    "Copy that token into a file called `.env` with the format:\n",
    "\n",
    "```\n",
    "DB_ACCESS_TOKEN=<THISISMYLONGSTRINGFROMDATABRICKS>\n",
    "```\n",
    "\n",
    "We will load this file later with the `python-dotenv` package to load the access token in such a way that it is not visible\n",
    "\n",
    "We'll start with just reading info from the workspace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working directory\n",
    "\n",
    "First, we will make sure that we are in the appropriate working directory where the `.env` file is, so that it can use the appropriate token. This directory should be the same as the directory that this notebook is in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Current working directory ****\n",
      "C:\\Users\\jeremr\\Documents\\GitHub\\MachineLearningSamples-PredictiveMaintenance\\Code\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('\\n**** Current working directory ****\\n%s\\n' %(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the token\n",
    "\n",
    "Assuming that is where you expected it to be, load the `.env` file. It should result in an environment variable `DB_ACCESS_TOK` added to your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the variables describing your databricks env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOMAIN = 'southcentralus.azuredatabricks.net'\n",
    "TOKEN = str.encode(os.getenv('DB_ACCESS_TOK')) ## convert to bytes as well\n",
    "BASE_URL = 'https://%s/api/2.0/' % (DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://southcentralus.azuredatabricks.net/api/2.0/'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_header = {\"Authorization\": b\"Basic \" + base64.standard_b64encode(b\"token:\" + TOKEN)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    BASE_URL + \"workspace/list\",\n",
    "    headers = my_header,\n",
    "    json={\n",
    "        \"path\": \"/Users/jeremr@microsoft.com/\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If successful\n",
    "\n",
    "`response` should be have a 200 value. You can then view it by converting it to json.\n",
    "\n",
    "The prior `get` command with `workspace/list` lists the libraries, files, and directories associated with that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objects': [{'object_type': 'LIBRARY',\n",
       "   'path': '/Users/jeremr@microsoft.com/graphframes-0.6.0-spark2.3-s_2.11'},\n",
       "  {'object_type': 'DIRECTORY', 'path': '/Users/jeremr@microsoft.com/Trash'},\n",
       "  {'object_type': 'DIRECTORY', 'path': '/Users/jeremr@microsoft.com/Github'},\n",
       "  {'object_type': 'DIRECTORY',\n",
       "   'path': '/Users/jeremr@microsoft.com/Rstudio-helpers'},\n",
       "  {'object_type': 'DIRECTORY',\n",
       "   'path': '/Users/jeremr@microsoft.com/TrainingMaterials'}]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a directory\n",
    "\n",
    "To make a directory, we just use a different REST method - we use a `post`, and we use `workspace/mkdirs`:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DB_HOME_DIR = \"/Users/jeremr@microsoft.com\"\n",
    "DIR_NAME = \"MachineLearningSamples-PredictiveMaintenance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    BASE_URL + \"workspace/mkdirs\",\n",
    "    headers = my_header,\n",
    "    json={\n",
    "        \"path\": '/'.join([DB_HOME_DIR, DIR_NAME])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import a notebook.\n",
    "\n",
    "To import a file, we just use a different method. It is still a `post`, but now, instead of using \"workspace/mkdirs\", we use \"workspace/import\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example\n",
    "\n",
    "This is just an example from the API documentation. It demonstrates a simple example, where the notebook code has already been converted to base64. This is a good example to run just to make sure that you can import a very simple notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(\n",
    "    BASE_URL + \"workspace/import\",\n",
    "    headers = my_header,\n",
    "    json = {\n",
    "      \"path\": '/'.join([DB_HOME_DIR, DIR_NAME, \"new-notebook\"]),\n",
    "      \"format\": \"SOURCE\",\n",
    "      \"language\": \"SCALA\",\n",
    "      \"content\": \"Ly8gRGF0YWJyaWNrcyBub3RlYm9vayBzb3VyY2UKcHJpbnQoImhlbGxvLCB3b3JsZCIpCgovLyBDT01NQU5EIC0tLS0tLS0tLS0KCg==\",\n",
    "      \"overwrite\": \"false\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(r)\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More complex example\n",
    "\n",
    "In this next example, I read in and process multiple notebooks, and send them via the `files` argument to `requests.post()`\n",
    "\n",
    "This requires that the files listed in `filenames` are in the current directory.\n",
    "\n",
    "There are some limitations on how big these files can be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = ['1_data_ingestion.ipynb',\n",
    " '2_feature_engineering.ipynb',\n",
    " '3_model_building.ipynb',\n",
    " '4_operationalization.ipynb']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_data_ingestion.ipynb\n",
      "/Users/jeremr@microsoft.com/MachineLearningSamples-PredictiveMaintenance/1_data_ingestion.ipynb\n",
      "<Response [200]>\n",
      "2_feature_engineering.ipynb\n",
      "/Users/jeremr@microsoft.com/MachineLearningSamples-PredictiveMaintenance/2_feature_engineering.ipynb\n",
      "<Response [200]>\n",
      "3_model_building.ipynb\n",
      "/Users/jeremr@microsoft.com/MachineLearningSamples-PredictiveMaintenance/3_model_building.ipynb\n",
      "<Response [200]>\n",
      "4_operationalization.ipynb\n",
      "/Users/jeremr@microsoft.com/MachineLearningSamples-PredictiveMaintenance/4_operationalization.ipynb\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "responses = []\n",
    "for fname in filenames:\n",
    "    print(fname)\n",
    "    remotepath = '/'.join([DB_HOME_DIR, DIR_NAME, fname])\n",
    "    print(remotepath)\n",
    "    with open(fname, 'rb') as f:\n",
    "        data = f.read()\n",
    "    ## post the request:\n",
    "    r = requests.post(\n",
    "                BASE_URL + \"workspace/import\",\n",
    "                headers = my_header,\n",
    "                data={\n",
    "                    \"path\": remotepath,\n",
    "                    \"format\": \"JUPYTER\",\n",
    "                    \"language\": \"PYTHON\",\n",
    "                    \"overwrite\": \"true\"              \n",
    "                },\n",
    "                files={\n",
    "                    \"content\": data\n",
    "                }\n",
    "              )\n",
    "    print(r)   \n",
    "    responses.append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:databricksapi]",
   "language": "python",
   "name": "conda-env-databricksapi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
